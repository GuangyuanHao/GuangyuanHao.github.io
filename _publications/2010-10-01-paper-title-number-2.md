---
title: "DSRGAN: Explicitly Learning Disentangled Representation of Underlying Structure and Rendering for Image Generation without Tuple Supervision"
collection: publications
permalink: 
excerpt: 'The paper was submitted to CVPR 2019 in Nov. 2018. The paper is about explicitly learning disentangled representation. Present methods like infoGAN, Beta-VAE aim to maximum mutual information between latent variables and images in nature, so those methods make each dimension of the input prior represent one of unknown factors as fully as possible. Therefore, factors that they disentangle are random and ambiguous and thus those models do not always disentangle factors people care. My project aims to explicitly learn disentangled representation so as to improve model interpretability.'
date: 2019-06-16
venue: Submitted to CVPR
paperurl: 
citation: 'Guangyuan Hao, Hongxing Yu, Weishi Zheng, &quot;DSRGAN: Explicitly Learning Disentangled Representation of Underlying Structure and Rendering for Image Generation without Tuple Supervision &quot; <i>Submitted to CVPR</i>, 2019.'
---
The paper was submitted to CVPR 2019 in Nov. 2018. The paper is about explicitly learning disentangled representation. Present methods like infoGAN, Beta-VAE aim to maximum mutual information between latent variables and images in nature, so those methods make each dimension of the input prior represent one of unknown factors as fully as possible. Therefore, factors that they disentangle are random and ambiguous and thus those models do not always disentangle factors people care. My project aims to explicitly learn disentangled representation so as to improve model interpretability.

[Download paper here](The link will be released after the paper is published)
