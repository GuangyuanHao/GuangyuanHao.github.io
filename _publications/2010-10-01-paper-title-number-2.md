---
title: "DSRGAN: Explicitly Learning Disentangled Representation of Underlying Structure and Rendering for Image Generation without Tuple Supervision"
collection: publications
permalink: 
excerpt: 'This paper aims to achieve a kind of mixture generation, i.e., generating images in a mixture domain containing the content of one domain and the style of another domain. A general method to mix kinds of entangled concepts from multiple domains for mixture generation is left for future work.'
date: 2019-07-16
venue: 
paperurl: 
citation: 'Guangyuan Hao, Hongxing Yu, Weishi Zheng, &quot;DSRGAN: Explicitly Learning Disentangled Representation of Underlying Structure and Rendering for Image Generation without Tuple Supervision &quot; <i>Submitted to CVPR</i>, 2019.'
---
This paper is about explicitly learning disentangled representation. Present methods like infoGAN, Beta-VAE aim to maximum mutual information between latent variables and images in nature, so those methods make each dimension of the input prior represent one of unknown factors as fully as possible. Therefore, factors that they disentangle are random and ambiguous and thus those models do not always disentangle factors people care. My project aims to explicitly learn disentangled representation so as to improve model interpretability.

[Download paper here](The link will be released after the paper is published.)
